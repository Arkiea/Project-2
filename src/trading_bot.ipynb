{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bZ1gCt22xiE"
      },
      "source": [
        "# Machine Learning Trading Bot\n",
        "\n",
        "This notebook aims to train machine learning algorithms to produce profitable trading signals.\n",
        "\n",
        "## Procedure\n",
        "\n",
        "1. Pull Bitcoin daily OHLCV candlestick data from Binance api\n",
        "2. Add technical indicators using `finta` and set up the entry/exit signals according to daily returns\n",
        "3. Split the data into training and testing datasets and scale the data\n",
        "4. Train machine learning models using `xgboost` on various combinations of technical indicators\n",
        "5. Make predictions using the testing data\n",
        "6. Review the classification report associated with the model predictions\n",
        "7. Plot the cumulative returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSfKph0g2xib"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "# in-built\n",
        "import itertools\n",
        "import os.path\n",
        "import datetime as dt\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "import hvplot.pandas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from finta import TA\n",
        "from binance import Client\n",
        "\n",
        "# machine learning imports\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ML models\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DYihUSN2xii"
      },
      "source": [
        "### Step 1: Download the OHLCV data into a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d36SIGw82xij"
      },
      "outputs": [],
      "source": [
        "# Instantiate Binance client\n",
        "client = Client()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtDWmQkH2xij"
      },
      "outputs": [],
      "source": [
        "# Create a function to download kline candlestick data from Binance\n",
        "def get_historical_data(base, quote='USDT'):\n",
        "    \"\"\" Download OHLCV data from Binance \"\"\"\n",
        "    klines = client.get_historical_klines(\n",
        "        base + quote,\n",
        "        Client.KLINE_INTERVAL_1DAY,\n",
        "        \"5 year ago UTC\",\n",
        "    )\n",
        "    # klines columns=['Open Time', 'Open', 'High', 'Low', 'Close', 'Volume', 'Close Time', 'Quote asset volume', 'Number of trades', 'Taker buy base asset volume', 'Taker buy quote asset volume', 'Ignore'])\n",
        "    # Keep OHLCV data\n",
        "    cols_ohlcv = ('open', 'high', 'low', 'close', 'volume')\n",
        "    df = pd.DataFrame((x[:6] for x in klines), columns=['timestamp', *cols_ohlcv])\n",
        "    df[[*cols_ohlcv]] = df[[*cols_ohlcv]].astype(float)\n",
        "    # Convert timestamp into date format\n",
        "    df['date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
        "    # Set date as index\n",
        "    df.set_index('date', inplace=True)\n",
        "    # Drop timestamp column\n",
        "    df.drop(columns='timestamp', inplace=True)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmAxOFGp2xik"
      },
      "outputs": [],
      "source": [
        "# Get OHLCV dataset from Binance\n",
        "today = (dt.datetime.today()).strftime('%Y-%m-%d')\n",
        "root_dir = os.path.join(Path().resolve(), '..')\n",
        "crypto = \"BTC\"\n",
        "try:\n",
        "    # load cached ohlcv_df from csv file\n",
        "    ohlcv_df = pd.read_csv(\n",
        "        os.path.join(root_dir, f\"ohlcv_df_{today}.csv\"),\n",
        "        index_col='date',\n",
        "        infer_datetime_format=True,\n",
        "        parse_dates=True,\n",
        "    )\n",
        "except FileNotFoundError:\n",
        "    # download ohlcv data from binance\n",
        "    ohlcv_df = get_historical_data(crypto)\n",
        "    # save the data to a csv\n",
        "    ohlcv_df.to_csv(os.path.join(root_dir, f\"ohlcv_df_{today}.csv\"))\n",
        "\n",
        "# Review the DataFrame\n",
        "ohlcv_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_eHkWn12xim"
      },
      "source": [
        "## Step 2: Generate trading signals using `finta`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBIKHYxI42Zv"
      },
      "outputs": [],
      "source": [
        "def add_TA_and_signal(ohlcv_df):\n",
        "    \"\"\" Add technical indicators and buy/sell signal to the input ohlcv dataframe \"\"\"\n",
        "    target_col = 'test'\n",
        "    # Add bollinger bands\n",
        "    bbands_df = TA.BBANDS(ohlcv_df)\n",
        "    # Add custom TA entry/exit signals using bollinger bands\n",
        "    bbands_df['close_vs_BB'] = np.select(\n",
        "        [\n",
        "            bbands_df['BB_UPPER'] < ohlcv_df['close'],\n",
        "            bbands_df['BB_LOWER'] > ohlcv_df['close'],\n",
        "        ],\n",
        "        [-1, 1],\n",
        "        default=0\n",
        "    )\n",
        "    # Add EMA\n",
        "    ema_df = pd.DataFrame(\n",
        "        [\n",
        "            TA.EMA(ohlcv_df, 5),\n",
        "            TA.EMA(ohlcv_df, 12),\n",
        "        ]\n",
        "    ).T\n",
        "    # ema_df['EMA_DIFFERENCE'] = np.where(ema_df.iloc[:,1] > ema_df.iloc[:,0], 1 , -1)\n",
        "    ema_df['EMA_DIFFERENCE'] = ema_df.iloc[:,1] - ema_df.iloc[:,0]\n",
        "    # calculate returns\n",
        "    returns_df = pd.DataFrame(ohlcv_df['close'].pct_change())\n",
        "    returns_df.columns = ['returns']\n",
        "    # set up entry/exit signals\n",
        "    returns_df[target_col] = np.where(returns_df['returns'] > 0, 1, -1)\n",
        "    returns_df[target_col] = returns_df[target_col].shift(-1)\n",
        "    # Add custom indicator that counts the consecutive number of green/red days\n",
        "    returns_df['consecutive'] = (\n",
        "        (\n",
        "            returns_df[target_col].groupby(\n",
        "                # true if the previous value is different from the the current\n",
        "                (returns_df[target_col] != returns_df[target_col].shift())\n",
        "                # cumulatively sum them up to categorise them into groups of the same values\n",
        "                .cumsum()\n",
        "            )\n",
        "            # count each value in the group starting from 1\n",
        "            .cumcount() + 1\n",
        "        # multiply each value by +/- 1 if the original was +ve or -ve\n",
        "        ) * np.where(returns_df[target_col] > 0, 1, -1)\n",
        "    # shift it back to normal because target_col is shifted -1\n",
        "    ).shift()\n",
        "    # Load sentiment analysis\n",
        "    sentiment_df = pd.read_csv(\n",
        "        '../Sentiment-analysis/BTC_2022-03-10_df.csv',\n",
        "        index_col='date',\n",
        "        parse_dates=True,\n",
        "    )\n",
        "    # drop source text columns\n",
        "    sentiment_df.drop(columns=['BTC_headline','BTC_desc'], inplace=True)\n",
        "    # fill missing days with value of 0.0\n",
        "    sentiment_df = sentiment_df.asfreq('D').fillna(0.0)\n",
        "\n",
        "    return pd.concat(\n",
        "        [\n",
        "            ohlcv_df,\n",
        "            TA.SMA(ohlcv_df, 4),\n",
        "            TA.SMA(ohlcv_df, 100),\n",
        "            bbands_df,\n",
        "            ema_df,\n",
        "            returns_df,\n",
        "            TA.RSI(ohlcv_df, 14),\n",
        "            TA.DMI(ohlcv_df),\n",
        "            TA.VWAP(ohlcv_df),\n",
        "            TA.PIVOT_FIB(ohlcv_df),\n",
        "            sentiment_df,\n",
        "        ],\n",
        "        axis='columns',\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BbOljWf7XVp"
      },
      "outputs": [],
      "source": [
        "# Add technical indicators and signal\n",
        "master_df = add_TA_and_signal(ohlcv_df).dropna()\n",
        "# Review the dataframe\n",
        "master_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXLHxoV42xi4"
      },
      "source": [
        "## Step 3: Split the data into training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poKcF3YH2xi5"
      },
      "outputs": [],
      "source": [
        "# Split training and test data\n",
        "\n",
        "# Segment the features from the target\n",
        "target_col = 'test'\n",
        "y = master_df[target_col]\n",
        "X = master_df.drop(columns=target_col)\n",
        "\n",
        "# number of weeks of test data\n",
        "num_weeks_test_data = 12\n",
        "# Select the ending period for the training data\n",
        "end_training = X.index.max() - DateOffset(weeks=num_weeks_test_data)\n",
        "# Generate the X_train, X_test, y_train and y_test DataFrames\n",
        "X_train = X.loc[: end_training]\n",
        "X_test = X.loc[end_training:]\n",
        "y_train = y.loc[: end_training]\n",
        "y_test = y.loc[end_training:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcP_SUEz2xi7"
      },
      "outputs": [],
      "source": [
        "# Scale the features DataFrames\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "X_scaler = StandardScaler()\n",
        "\n",
        "# Apply the scaler model to fit the X-train data\n",
        "X_scaler.fit(X_train)\n",
        "\n",
        "# Transform the X_train and X_test DataFrames using the X_scaler\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_RJuxVn2xi8"
      },
      "source": [
        "## Step 4: Train a machine learning model using `xgboost`, fitting it the training data and make predictions based on the testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RL3di1l2xi8"
      },
      "outputs": [],
      "source": [
        "# create the Xgboost base model\n",
        "model = xgb.XGBClassifier(\n",
        "    n_estimators=10,\n",
        "    max_depth=5,\n",
        "    objective='reg:logistic',\n",
        "    learning_rate=0.1,\n",
        "    random_state=1,\n",
        ")\n",
        "# fit the model using the training data\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# make predictions\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "# Review the model's predicted values\n",
        "predictions[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binYERSg2xi9"
      },
      "source": [
        "## Step 5: Review the classification report associated with the `xgboost` model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYsVc-_a2xi9"
      },
      "outputs": [],
      "source": [
        "# Use a classification report to evaluate the model using the predictions and testing data\n",
        "report = classification_report(y_test, predictions, output_dict=True)\n",
        "\n",
        "# Print the classification report\n",
        "print(json.dumps(report, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYEr2C62xi9"
      },
      "source": [
        "## Step 6: Create a predictions DataFrame that contains columns for \"predictions\" values, \"daily returns\", and \"model returns\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "821QQuEV2xi-"
      },
      "outputs": [],
      "source": [
        "# Calculate the model's returns\n",
        "predictions_df = X_test.copy()\n",
        "# Add y_test to the DataFrame\n",
        "predictions_df['y_test'] = y_test\n",
        "# Add the model predictions to the DataFrame\n",
        "predictions_df['predictions'] = predictions\n",
        "# Add the actual returns to the DataFrame\n",
        "predictions_df['daily returns'] = master_df['returns']\n",
        "# Add the strategy returns to the DataFrame\n",
        "predictions_df['model returns'] = predictions_df['daily returns'] * predictions_df['predictions'].shift()\n",
        "\n",
        "# Review the DataFrame\n",
        "display(predictions_df.head())\n",
        "display(predictions_df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKsXKqJc2xi-"
      },
      "source": [
        "## Step 7: Create a cumulative return plot that shows the actual returns vs. the strategy returns. Save a PNG image of this plot. This will serve as a baseline against which to compare the effects of tuning the trading algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U37oGPb2xi-"
      },
      "outputs": [],
      "source": [
        "# Plot the cumulative daily returns versus the cumulative model returns\n",
        "cumulative_returns = (1 + predictions_df[['daily returns', 'model returns']]).cumprod()\n",
        "fig = cumulative_returns.plot(title='Daily BTC Returns VS XGBoost Model Returns').get_figure()\n",
        "# save the plot as an image\n",
        "fig.savefig('xgb_model_returns.png', bbox_inches='tight')\n",
        "# Display as a hvplot\n",
        "cumulative_returns.hvplot(title='Daily BTC Returns VS XGBoost Model Returns')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1DYihUSN2xii"
      ],
      "name": "trading_bot.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "1b8d2b8336021f1625f43bea189960d31b14cee4ef5dd500cff33471658fed61"
    },
    "kernelspec": {
      "display_name": "Python (dev)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
